# Peer Feedback Report: {displayName}

**Period:** {fromDate} to {toDate}
**Team:** {team}
**Report Generated:** {generatedDate}
**Data Sources:** {sourcesList}

---

## Executive Summary

{2-3 paragraphs telling the story of this person's contributions during the review period. Lead with the most significant narrative arc — what did they build, ship, or drive? Follow with supporting themes. Weave in specific numbers naturally as evidence, but don't lead with statistics. A reader should come away understanding what this person did and why it mattered.}

---

## Contribution Narratives

{3-5 major work arcs told as full case studies. Each narrative should reconstruct the full story: context → action → impact. Reference specific artifacts inline (PR links, ticket keys, document titles). Include supporting data where it adds weight. These are the centerpiece of the report.}

### {Narrative 1 Title — e.g., "Leading the Migration to the New Search Infrastructure"}

{Full narrative: What was the problem or opportunity? What did they do? How did they approach it? What was the result? Reference specific PRs, tickets, Confluence pages, Slack discussions, etc. Include quantitative evidence naturally within the story.}

### {Narrative 2 Title}

{...}

### {Narrative 3 Title}

{...}

{Add more narratives as warranted by the data. Scale to data richness — rich data gets 4-5 narratives, sparse data gets 2-3 focused ones.}

---

## Thematic Highlights

{Narrative-led highlights organized by theme. Each theme should tell a story, not list statistics. Use specific examples and connect dots across sources.}

### Technical Delivery

{Story of their technical output — what they shipped, how they approached it, patterns in their engineering work. Reference specific PRs, tickets, and features with context about why they mattered.}

### Collaboration & Teamwork

{Story of how they worked with others — cross-team contributions, PR review engagement, collaborative efforts. Connect Slack discussions to code reviews to shared documentation.}

### Leadership & Initiative

{Story of how they drove outcomes beyond their individual tickets — ADDs authored, architectural decisions, presentations, mentoring signals, process improvements.}

### Documentation & Knowledge Sharing

{Story of their knowledge contributions — Confluence pages, technical docs, runbooks, guides. Note the impact: did these unblock others or codify team knowledge?}

### Communication & Engagement

{Story of their communication patterns — Slack engagement, proactive updates, incident response, cross-team conversations. Note depth of participation and helpfulness.}

---

## Small but Meaningful Contributions

{Smaller items that demonstrate attention to detail, team citizenship, or initiative. Provide context for why each matters:}
- {Contribution with context — e.g., "Fixed a flaky test in the search service that had been causing intermittent CI failures for the team"}
- {Contribution with context}
- {Contribution with context}

---

## Growth & Impact Observations

{Narrative about this person's trajectory during the review period:}
- {Observation about expanding scope, with evidence}
- {Observation about new skills or domains, with evidence}
- {Observation about increasing complexity of work}
- {Observation about impact on team productivity}

---

## Data Summary

| Source | Key Metrics |
|--------|------------|
| GitHub | {X PRs authored, Y PRs reviewed, Z repositories} |
| Jira | {X tickets completed, Y story points, Z bugs fixed} |
| Confluence | {X pages created, Y pages modified, Z ADDs} |
| Google Drive | {X documents created} |
| Slack | {X messages, Y channels active, Z thread replies} |

---

{BEGIN CONDITIONAL: Include only when C2 competency framework is provided}

## C2 Competency Mapping

{For each competency in the C2 framework, provide an insight-driven assessment structured as Good Things / Things to Improve — matching the C2 form layout so content can be directly transposed.}

### {Competency 1 Name — e.g., "Craftsmanship"}

**Good Things:**
- {Qualitative insight about a positive behavior — lead with the quality/skill demonstrated, explain why it matters, use specific artifacts as supporting evidence. Should be directly copy-pasteable into the C2 "Good Things" field.}
- {Another insight — e.g., "Demonstrates strong ownership of technical quality — proactively identified and fixed a class of flaky tests (PRs #1234, #1237) without being asked, reducing CI noise for the whole team."}
- {Another insight if data supports it}

**Things to Improve:**
- {Qualitative insight about a growth area — lead with the specific opportunity, ground in evidence when possible, frame constructively. Should be directly copy-pasteable into the C2 "Things to Improve" field.}
- {Another growth area — e.g., "Cross-team communication during large initiatives could be more proactive — the Temporal rollout Slack threads show coordination happened mostly reactively when issues arose, rather than with regular status updates."}
- {If insufficient data: "The available data does not surface clear growth areas for this competency. Consider asking: [specific question to help identify growth areas]"}

**Rubric Alignment:** {Quote the specific criteria text from the rubric level that best matches the observed behaviors. Identify the level (Building/Proficient/Advanced/Outstanding).}

**Suggested Rating:** {Strong | Pretty Good | Could Improve}

### {Competency 2 Name}

{...same structure...}

{Repeat for all competencies in the C2 framework.}

{END CONDITIONAL}

---

## Questions for You

{8-12 targeted, data-grounded questions designed to fill gaps in the observable data. Each question should reference something specific from the data to anchor it. Questions help the feedback writer add subjective context that automated data gathering cannot capture.}

### Code Quality & Technical Judgment
{2-3 questions about technical decisions, code quality, debugging approach, or architectural thinking that can't be fully assessed from PR metadata alone.}
1. {Question referencing specific technical work — e.g., "You reviewed 3 of their PRs in the search service rewrite. How would you characterize the quality of their technical approach and code organization?"}
2. {Question}

### Communication & Collaboration
{2-3 questions about interpersonal dynamics, meeting contributions, or collaboration quality that Slack/GitHub data only partially captures.}
3. {Question referencing specific collaborative work}
4. {Question}

### Reliability & Execution
{1-2 questions about dependability, follow-through, or execution patterns not fully visible in ticket completion data.}
5. {Question referencing specific delivery patterns}

### Growth & Development
{1-2 questions about learning trajectory, mentoring, or skill development.}
6. {Question referencing observed growth signals}

### Strengths & Growth Areas
{2-3 questions that directly ask the feedback writer to identify both what this person does well and where they could grow, framed around specific observed work. Essential for filling C2 "Good Things" / "Things to Improve" fields.}
7. {Question — e.g., "They led the Temporal adoption which required significant cross-team coordination. Were there moments where communication or coordination could have gone smoother? What would you suggest they do differently?" [Communication, SDLC]}
8. {Question targeting a competency where "Things to Improve" data was insufficient}

### Filling in the Gaps
{1-2 questions targeting specific data gaps or areas with thin coverage.}
9. {Question about an area with limited data — e.g., "We had limited Confluence data for this period. Can you speak to their documentation practices or knowledge sharing?"}

{When C2 framework is provided, tag each question with the relevant competency name in brackets, e.g., [Craftsmanship], [Communication]. Every competency where "Things to Improve" lacked data should have at least one targeted question.}

---

*Report generated by salsify-peer-work-extract plugin*
