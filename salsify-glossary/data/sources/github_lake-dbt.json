[
  {
    "acronym": "lake-dbt",
    "name": "Lake DBT - Salsify Data Lake DBT Project",
    "description": "The DBT (Data Build Tool) project for Salsify's Data Lake. Contains SQL models that transform raw data into analytics-ready datasets using the medallion architecture (bronze, silver, gold tiers). Orchestration is handled by Dagster in the lake-jobs repository. Uses AWS Athena as the query engine.",
    "category": "GitHub Repos",
    "relatedTerms": ["lake-jobs", "Dagster", "DBT", "Athena", "Medallion Architecture"],
    "links": [
      {"url": "https://github.com/salsify/lake-dbt", "description": "GitHub Repository"}
    ]
  },
  {
    "acronym": "lake-jobs",
    "name": "Lake Jobs - Data Lake Pipeline Orchestration",
    "description": "The Dagster-based pipeline orchestration repository for Salsify's Data Lake. Handles job scheduling, execution, and monitoring of DBT models and other data pipelines. Manages deployment to development (staging) and production environments.",
    "category": "GitHub Repos",
    "relatedTerms": ["lake-dbt", "Dagster", "DBT"],
    "links": [
      {"url": "https://github.com/salsify/lake-jobs", "description": "GitHub Repository"},
      {"url": "http://lake-jobs-prod.lake.salsify.com/", "description": "Dagster Production UI"}
    ]
  },
  {
    "acronym": "Dagster",
    "name": "Dagster - Data Orchestration Platform",
    "description": "The open-source data orchestration platform used by Salsify to schedule and manage Data Lake pipelines. Provides job execution, monitoring, backfill capabilities, and DAG visualization. The production instance is accessible at lake-jobs-prod.lake.salsify.com.",
    "category": "Engineering",
    "relatedTerms": ["lake-dbt", "lake-jobs", "DAG"],
    "links": [
      {"url": "http://lake-jobs-prod.lake.salsify.com/", "description": "Dagster Production UI"},
      {"url": "https://dagster.io/", "description": "Dagster Official Documentation"}
    ]
  },
  {
    "acronym": "DBT",
    "name": "DBT - Data Build Tool",
    "description": "An open-source ELT (Extract, Load, Transform) tool used by Salsify's Data Lake team. Enables analysts and engineers to transform raw data using SQL models, apply version control, add documentation, and run tests. Each SQL file generates a table or view in the data lake.",
    "category": "Engineering",
    "relatedTerms": ["lake-dbt", "Medallion Architecture", "Athena"],
    "links": [
      {"url": "https://docs.getdbt.com/docs/introduction", "description": "DBT Official Documentation"}
    ]
  },
  {
    "acronym": "Medallion Architecture",
    "name": "Medallion Architecture - Data Lake Tier Structure",
    "description": "A data lake design pattern used by Salsify organizing data into three quality tiers: Bronze (raw data, minimal transformation), Silver (cleaned and conformed data, business logic applied), and Gold (aggregated, analytics-ready data). Each tier is represented by folders within the DBT models directory.",
    "category": "Engineering",
    "relatedTerms": ["lake-dbt", "Bronze", "Silver", "Gold"],
    "links": [
      {"url": "https://salsify.atlassian.net/wiki/spaces/DP/pages/260116021361/Architecture+and+Naming+Convention", "description": "Data Lake Architecture Documentation"}
    ]
  },
  {
    "acronym": "Bronze",
    "name": "Bronze Layer - Raw Data Tier",
    "description": "The first tier in the medallion architecture containing raw data with minimal transformation. In lake-dbt, bronze models are typically materialized as views and serve as the initial staging layer for data from sources like Dandelion. Naming convention: bronze_{source}__{entity}.",
    "category": "Engineering",
    "relatedTerms": ["Medallion Architecture", "Silver", "Gold", "lake-dbt"]
  },
  {
    "acronym": "Silver",
    "name": "Silver Layer - Curated Data Tier",
    "description": "The second tier in the medallion architecture containing cleaned, conformed, and transformed data. Silver models apply business logic, handle data quality, and are typically materialized as views. Naming convention: silver_{domain}__{entity}.",
    "category": "Engineering",
    "relatedTerms": ["Medallion Architecture", "Bronze", "Gold", "lake-dbt"]
  },
  {
    "acronym": "Gold",
    "name": "Gold Layer - Analytics-Ready Data Tier",
    "description": "The third and final tier in the medallion architecture containing aggregated, analytics-ready data optimized for reporting and consumption. Gold models are typically materialized as tables and may use incremental loading. Naming convention: gold_{domain}__{entity}.",
    "category": "Engineering",
    "relatedTerms": ["Medallion Architecture", "Bronze", "Silver", "lake-dbt"]
  },
  {
    "acronym": "DAG",
    "name": "DAG - Directed Acyclic Graph",
    "description": "In the context of DBT and data pipelines, a DAG represents the dependency relationships between data models. DBT automatically builds a DAG from model references (ref() function) to ensure models are executed in the correct order. Dagster visualizes and executes pipelines based on DAG structure.",
    "category": "Engineering",
    "relatedTerms": ["lake-dbt", "DBT", "Dagster"]
  },
  {
    "acronym": "Athena",
    "name": "AWS Athena - Query Service",
    "description": "AWS's serverless query service used by Salsify's Data Lake as the execution engine for DBT models. Queries are limited to 60 minutes, so models should complete within 20 minutes for safety margin. Data is stored in S3 and accessed via the salsify-data-eng AWS account.",
    "category": "Engineering",
    "relatedTerms": ["lake-dbt", "DBT"],
    "links": [
      {"url": "https://us-east-1.console.aws.amazon.com/athena/home?region=us-east-1#/query-editor", "description": "AWS Athena Console"}
    ]
  },
  {
    "acronym": "EC Digital Shelf",
    "name": "EC Digital Shelf - Enhanced Content Analytics",
    "description": "A data domain in lake-dbt tracking Enhanced Content (EC) performance across retailer digital shelves. Includes metrics like ec_view_count, module_view_count, module_interaction_count, and ATC (Add to Cart) conversion rates. Used to measure the effectiveness of published enhanced content.",
    "category": "Product",
    "relatedTerms": ["Enhanced Content", "ATC", "lake-dbt"]
  },
  {
    "acronym": "ATC",
    "name": "ATC - Add to Cart",
    "description": "An analytics event in the Enhanced Content pipeline indicating when a user adds a product to their shopping cart. Used as a conversion metric to measure the effectiveness of Enhanced Content. Tracked both with and without EC views to compare conversion rates.",
    "category": "Product",
    "relatedTerms": ["EC Digital Shelf", "Enhanced Content", "Conversion Rate"]
  },
  {
    "acronym": "CSID",
    "name": "CSID - Customer Session ID",
    "description": "A UUID assigned to users visiting product pages with Enhanced Content. Used to track unique visitors and calculate metrics like unique page views, unique ATC events, and conversion rates across the EC Digital Shelf analytics pipeline.",
    "category": "Engineering",
    "relatedTerms": ["EC Digital Shelf", "Enhanced Content"]
  },
  {
    "acronym": "Holdback",
    "name": "Holdback - A/B Testing Control Group",
    "description": "An A/B testing mechanism in Enhanced Content analytics where some page sessions have EC content withheld (hidden) to measure its impact. Enables comparison of conversion rates between users who see EC content versus those in the holdback group.",
    "category": "Product",
    "relatedTerms": ["EC Digital Shelf", "Enhanced Content", "ATC"]
  },
  {
    "acronym": "Backfill",
    "name": "Backfill - Historical Data Reprocessing",
    "description": "The process of reprocessing historical data through incremental models when schema changes occur or source data is updated. Initiated via the Dagster UI by selecting incremental models and launching a backfill for the desired time range. May require downstream service backfills.",
    "category": "Engineering",
    "relatedTerms": ["lake-dbt", "Dagster", "Incremental Model"]
  },
  {
    "acronym": "Incremental Model",
    "name": "Incremental Model - Partition-Based Data Loading",
    "description": "A DBT materialization strategy that only processes new or changed data rather than rebuilding entire tables. Uses date partitioning (min_date_inclusive, max_date_exclusive) for efficient daily processing. Requires backfills when upstream changes affect historical data.",
    "category": "Engineering",
    "relatedTerms": ["lake-dbt", "DBT", "Backfill"]
  },
  {
    "acronym": "salsify-data-eng",
    "name": "salsify-data-eng - Data Engineering AWS Account",
    "description": "The AWS account containing Salsify's Data Lake infrastructure. Developers access it with data-analyst or data-eng IAM roles via Okta SSO. Contains Athena workgroups, S3 data storage, and development schemas prefixed with dev_{username}.",
    "category": "Engineering",
    "relatedTerms": ["lake-dbt", "Athena"]
  }
]
