[
  {
    "acronym": "lake-jobs",
    "name": "Lake Jobs",
    "description": "Data processing orchestrator built on Dagster for the Salsify data lake. Manages data pipelines including ETL/ELT operations, YAML-based datasource processing, customer data feeds, Salesforce data, and snapshots. Integrates with lake-dbt for transformations and datasource-configs for configuration management.",
    "category": "GitHub Repos",
    "links": [
      {"url": "https://github.com/salsify/lake-jobs", "description": "GitHub Repository"},
      {"url": "https://engineering.internal.salsify.com/docs/default/Component/lake-jobs", "description": "Engineering Portal Documentation"}
    ]
  },
  {
    "acronym": "YAML ETL",
    "name": "YAML ETL",
    "description": "A module in lake-jobs that processes disparate data sources into partitioned Athena tables. Reads YAML datasource configs from the datasource-configs repository, dynamically generates Dagster assets, and creates schedules for each datasource. Uses Dask for parallel processing of CSV and JSONL files.",
    "category": "Engineering",
    "links": [
      {"url": "https://github.com/salsify/lake-jobs/blob/main/docs/yaml_etl.md", "description": "YAML ETL Documentation"}
    ]
  },
  {
    "acronym": "datasource-configs",
    "name": "Datasource Configs",
    "description": "Repository containing YAML configuration files that define data sources for the Salsify data lake. Used by lake-jobs YAML ETL module to dynamically generate Dagster assets and processing schedules for each datasource.",
    "category": "GitHub Repos",
    "links": [
      {"url": "https://github.com/salsify/datasource-configs", "description": "GitHub Repository"}
    ]
  },
  {
    "acronym": "lake-dbt",
    "name": "Lake DBT",
    "description": "Repository containing dbt (data build tool) models for data transformations in the Salsify data lake. Used by lake-jobs ELT module to run SQL-based transformations on data stored in Athena.",
    "category": "GitHub Repos",
    "links": [
      {"url": "https://github.com/salsify/lake-dbt", "description": "GitHub Repository"}
    ]
  },
  {
    "acronym": "MAO",
    "name": "Monitoring, Alerting, and Observability",
    "description": "Framework in lake-jobs for tracking data pipeline health. Emits events to New Relic for job runs and dbt executions, including metrics like execution duration, rows affected, data scanned, and job status. Supports cost estimation for Athena queries and SLA analysis.",
    "category": "Engineering",
    "links": [
      {"url": "https://onenr.io/0oQD3yNm0jy", "description": "Dagster New Relic Dashboard"}
    ]
  },
  {
    "acronym": "CDF",
    "name": "Customer Data Feeds",
    "description": "A Dagster code location/module in lake-jobs that handles customer-specific data feed processing for the Salsify data lake.",
    "category": "Engineering",
    "links": [
      {"url": "https://github.com/salsify/lake-jobs", "description": "GitHub Repository"}
    ]
  },
  {
    "acronym": "salsify-data-eng",
    "name": "Salsify Data Engineering AWS Account",
    "description": "The AWS account used by the Data Engineering team for data lake infrastructure, including Athena, S3 buckets, and lake-jobs deployments. Accessed via Okta SSO with roles like data-analyst.",
    "category": "Engineering",
    "links": [
      {"url": "https://salsify.atlassian.net/wiki/spaces/ENG/pages/94212161/Authenticate+AWS+CLI+using+Okta+SSO", "description": "AWS CLI Authentication Guide"}
    ]
  },
  {
    "acronym": "YMD",
    "name": "Year-Month-Day Partitioning",
    "description": "A partitioning scheme (YYYY/MM/DD) used for organizing larger data sources in the Salsify data lake S3 buckets. Used by datasources like NewRelic and syndication metrics.",
    "category": "Engineering",
    "links": [
      {"url": "https://github.com/salsify/lake-jobs/blob/main/docs/yaml_etl.md", "description": "YAML ETL Documentation"}
    ]
  }
]
